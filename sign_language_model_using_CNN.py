# -*- coding: utf-8 -*-
"""Sign_Language_Model_B64_E10_Iter05.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ylZ6xRFBJI4nIluuko7hnEqeSWJcpc-4

# Singn_Language_Detection_Using_CNN

# **Importing Libraries**

*   **Numpy and Pandas**
*   **Matplotlib and Seaborn**
*   **TensorFlow and Keras**
*   **Scikit-learn**
"""

# Importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# TensorFlow and Keras for building and training the neural network
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import regularizers
from tensorflow.keras.utils import to_categorical

# Sklearn for performance metrics and preprocessing
from sklearn.metrics import classification_report, confusion_matrix, f1_score
from sklearn.preprocessing import LabelBinarizer

# Suppress the specific warning
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module='tensorflow.keras.preprocessing.image')

"""# **Exploring and Visualizing Dataset**
*   Loading and Checking Data
*   Visualizing Class Distribution





"""

# Loading and Checking Train Data
train_data_reference = pd.read_csv('/content/sign_mnist_train.csv')
print('train_data_referenceshape :', train_data_reference.shape)
print('Null_Values :' ,train_data_reference.isnull().sum().sum()) # To check null values in the data set
train_data_reference.info()

train_data_reference.head()

# Loading and Checking Test Data
test_data_reference = pd.read_csv('/content/sign_mnist_test.csv')
print('test_data_reference_shape :', test_data_reference.shape)
print('Null_Values :' ,test_data_reference.isnull().sum().sum()) # To check null values in the data set
test_data_reference.info()

test_data_reference.head()

#Visualizing Train and Test Data
#Two subplots (1 row, 2 columns)
fig, axs = plt.subplots(1, 2, figsize=(12, 6))

# Plot for training data
sns.countplot(data=train_data_reference, x="label", ax=axs[0])
axs[0].set_title('Training Data')

# Plot for testing data
sns.countplot(data=test_data_reference, x="label", ax=axs[1])
axs[1].set_title('Testing Data')

# Visualizinf the data
plt.tight_layout()
plt.show()

"""# **Loading and Preprocessing the Dataset**

"""

# Function to load data from CSV
def load_csv_data(csv_file):
    data = pd.read_csv(csv_file)
    labels = data.iloc[:, 0]  # Extracting the Lables
    images = data.iloc[:, 1:].values  # Extracting the Pixels
    images = images.reshape(-1, 28, 28, 1) # Reshape to 28x28
    return images, labels

# Load training and testing data
train_images, train_labels = load_csv_data('/content/sign_mnist_train.csv')
test_images, test_labels = load_csv_data('/content/sign_mnist_test.csv')

# Normalize the images with maximum pixel value 255.0
train_images = train_images / 255.0
test_images = test_images / 255.0

# Label Encoding using LabelBinarizer()
label_binarizer = LabelBinarizer()
train_labels = label_binarizer.fit_transform(train_labels)
test_labels = label_binarizer.transform(test_labels)

print("train_image_shape :" ,train_images.shape)
print("train_image_datatype :" , train_images.dtype)
print("train_label_shape :" ,train_labels.shape)
print("train_label_datatype :" , train_labels.dtype)
print("test_image_shape :" ,test_images.shape)
print("test_image_datatype :" , test_images.dtype)
print("test_label_shape :" ,test_labels.shape)
print("test_label_datatype :" , test_labels.dtype)

"""# **Data Augmentation**"""

# Data Augmentation
datagen = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=False
)

"""# **Convolutional Neural Network Model Architecture**

**Model Architecture:**
*   Convolutional Layers
*   Normalization and Pooling
*   Flattening and Dense Layers
*   Batch Normalization and Dropout
*   Output Layer
"""

# Model Building
model = Sequential([
    Input(shape=(28, 28, 1)),
    Conv2D(32, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.2),

    Conv2D(64, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.2),

    Conv2D(128, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.2),

    Flatten(),
    Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    BatchNormalization(),
    Dropout(0.5),

    Dense(24, activation='softmax')
])

"""# **Compiling the Convolutional Neural Network Model**"""

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

"""# **Custom Callback for F1 Score**

"""

# Custom callback to calculate F1 score for each epoch
class F1ScoreCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        predictions = model.predict(test_images)
        predicted_labels = np.argmax(predictions, axis=1)
        true_labels = np.argmax(test_labels, axis=1)
        f1 = f1_score(true_labels, predicted_labels, average='weighted')
        print(f"Epoch {epoch+1} - F1 Score: {f1:.2f}")

"""# **Training the Convolutional Neural Network Model**"""

# Model Training
history = model.fit(datagen.flow(train_images, train_labels, batch_size=64),
                    validation_data=(test_images, test_labels),
                    epochs=10,
                    callbacks=[F1ScoreCallback()])

"""# **Evaluating the Trained Convolutional Neural Network Model**"""

# Model Evaluation
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"Test accuracy: {test_acc}")

"""# **Classification Report and Confusion Matrix for Model Evaluation**



"""

# Classification Report and Confusion Matrix
predictions = model.predict(test_images)
predicted_labels = np.argmax(predictions, axis=1)
true_labels = np.argmax(test_labels, axis=1)

# Convert label binarizer classes to list of strings
target_names = [str(class_label) for class_label in label_binarizer.classes_]

print(classification_report(true_labels, predicted_labels, target_names=target_names))

#Confusion_Matrix
cm = confusion_matrix(true_labels, predicted_labels)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_binarizer.classes_, yticklabels=label_binarizer.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

def plot_metrics(history):
    plt.figure(figsize=(12, 5))

    # Plotting model accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plotting model loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

plot_metrics(history)

"""# **Model Saving**"""

# Save the model
model.save('Sign_Language_Model_B64_E10_Iter05.h5')